<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>MAS-2025</title>
  <meta content="MAS-2025" name="description">
  <meta content name="keywords">

  <!-- Favicons -->
  <!-- <link href="assets/img/favicon.png" rel="icon"> -->
  <link href="assets/img/icml-navbar-logo.svg" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i, Raleway:300,300i,400,400i,500,500i,600,600i,700,700i, Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <style>
    li {
      margin: 10px 10px 10px 10px;
    }
  </style>

  <style>
    .interactive-panel {
      padding: 10px;
      border: 1px solid #ddd;
      border-radius: 5px;
      transition: all 0.3s ease;
      margin-bottom: 15px;
      /* Add margin to the bottom of each panel */
    }

    .interactive-panel:hover {
      border-color: #007bff;
      box-shadow: 0 0 10px rgba(0, 123, 255, 0.5);
      cursor: pointer;
    }
  </style>

  <style>
    /* CSS for coloring schedule rows */
    .morning-session {
      background-color: #fff4d5;
      /* Light yellow for morning sessions */
    }

    .afternoon-session {
      background-color: #dbe7f2;
      /* Light blue for afternoon sessions */
    }

    .evening-session {
      background-color: #e9daeb;
      /* Light purple for evening sessions */
    }

    .night-session {
      background-color: #f9f0ed;
      /* Light purple for evening sessions */
    }
  </style>

</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center  header-transparent ">
    <div class="container d-flex align-items-center justify-content-between">

      <div class="logo">

        <!-- Uncomment below if you prefer to use an image logo -->
        <h1><a href="https://llmagents.github.io/" target="blank_"> </a></h1>

        <!-- <img src="assets/img/logo.png" alt="TamingLLM@SIGDIAL & INLG 2023"> -->

        <!-- <p style="margin : 0; padding-top:0; padding-left: 80px; padding-bottom:0;  line-height:0; font-size: 10px; text-align: center;" class="green-text">
        May 26-28, 2022 ,  Dublin
      </p> -->
      </div>

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <!-- <li><a class="nav-link scrollto" href="#about">About</a></li> -->
          <li><a class="nav-link scrollto" href="#topics">Call For Papers</a></li>
          <li><a class="nav-link scrollto" href="#SubmissionGuide">Submission Guide</a></li>
          <!-- <li><a class="nav-link scrollto" href="#cfp">Call for
              Papers</a></li>
          <li><a class="nav-link scrollto" href="#accepted-papers">Accepted
              Papers</a></li> -->
          <li><a class="nav-link scrollto" href="#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="#challenge">Challenge</a></li>
          <li><a class="nav-link scrollto" href="#speaker">Speakers</a></li>
          <!-- <li><a class="nav-link scrollto" href="#panelist">Panelists</a></li> -->
          <li><a class="nav-link scrollto" href="#org">Organization</a></li>
          <!-- <li><a class="nav-link scrollto" href="#reviewers">Reviewers</a></li> -->
          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>
          <li><a class="nav-link scrollto" href="#sponsors">Sponsors</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->

  <section id="hero" class="d-flex flex-column justify-content-end align-items-center">
    <div id="heroCarousel" data-bs-interval="5000" class="container carousel carousel-fade" data-bs-ride="carousel">

      <!-- Slide 1 -->
      <div class="carousel-item active">
        <div class="carousel-container">
          <div style="text-align: center;">
            <!-- <h2 style="font-size: 30px; color: #a2d2f4;" class="animate__animated animate__fadeInDown">ICML 2025
              Workshop</h2> -->
            <h2 style="font-size: 60px; color: #daa520;" class="animate__animated animate__fadeInDown">ICML 2025
              Workshop on MAS
            </h2>
            <!-- <h2 style="font-size: 30px; color: #ffffff;" class="animate__animated animate__fadeInDown">Forty-Second
              International Conference on Machine Learning</h2> -->
            <h3 style="font-size: 50px; color: #86b2ef;; margin-top: 0px; font-weight: 700;"
              class="animate__animated animate__fadeInDown">
              Multi-Agent Systems in the Era of Foundation Models: Opportunities, Challenges and Futures</h3>
            <!-- <h3 style="font-size: 30px; color: #44cbd6; margin-top: 0px;" class="animate__animated animate__fadeInDown">
              July 13, 2025</h3> -->
          </div>
          <p class="animate__animated animate__fadeInDown">
            Links:
            <a href="https://icml.cc/" target="_blank" style="color: lightgrey;">ICML 2025 | </a>
            <a href="https://openreview.net/group?id=ICML.cc/2025/Workshop/MAS&referrer=%5BHomepage%5D(%2F)"
              target="_blank" style="color: lightgrey;">OpenReview</a>
            <!-- <a href="https://icml.cc/Conferences/2025" target="_blank" style="color: lightgrey;">ICML 2025</a> -->
          </p>

          <p class="animate__animated fanimate__adeInUp">

          </p>
        </div>
      </div>

    </div>

    <svg class="hero-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
      viewBox="0 24 150 28 " preserveAspectRatio="none">
      <defs>
        <path id="wave-path" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z">
      </defs>
      <g class="wave1">
        <use xlink:href="#wave-path" x="50" y="3" fill="rgba(255,255,255, .1)">
      </g>
      <g class="wave2">
        <use xlink:href="#wave-path" x="50" y="0" fill="rgba(255,255,255, .2)">
      </g>
      <g class="wave3">
        <use xlink:href="#wave-path" x="50" y="9" fill="#fff">
      </g>
    </svg>

  </section><!-- End Hero -->

  <main id="main">

    <section id="about" class="contact">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
          </div>
          <div class="col-md-9">
            <p style="font-size: 20px; margin-bottom: 20px;">
              <strong>Motivation.</strong> The scaling of model parameters has unlocked the groundbreaking capabilities
              of foundation
              models. Likewise, in human society, scaling and collaboration across individuals,
              organizations, companies, and nations amplify collective intelligence to unprecedented levels, enabling
              remarkable achievements that would be impossible for individuals alone, such as space exploration and
              autonomy. Could this principle of scaling also apply to the growth in the number of agents?
              Multi-agent systems may offer a promising path forward. By progressively integrating more agents,
              multi-agent systems can activate diverse functionalities within these
              foundation model-powered generalist agents and coordinate a broader range of complementary
              functionalities. This synergy fosters improved problem-solving, adaptability, and decision-making
              capabilities. As the multi-agent system scales, it has a huge potential to achieve enhanced capabilities
              and tackle increasingly complex tasks, offering a promising solution toward the ultimate goal of achieving
              artificial general intelligence (AGI).
            </p>
            <p style="font-size: 20px; margin-bottom: 20px;">
              <strong>Background.</strong> With the advent of large foundation models, including large language models
              (LLMs)
              and visual language models (VLMs), research in multi-agent systems has progressed from relying on
              specialized models to harnessing the versatile capabilities of more generalized LLM/VLM-powered agents. As
              a community, we are embracing this paradigm shift to develop general-purpose multi-agent systems. These
              systems aim to leverage collaboration to enhance individual agent capabilities,
              ultimately advancing toward sophisticated AI agents that can serve as versatile human assistants. This
              field has already seen numerous promising developments. For instance, multi-agent systems for social
              simulation are used to generate diverse synthetic data, further enhancing the
              capabilities of foundation models and also investigating social behaviors. Moreover, multi-agent systems
              emulate human standards of processing through agentic workflows, significantly advancing a wide range of
              complex and urgent real-world applications, including mathematics, software development, web
              and mobile operations, embodied manipulation, and navigation, among others. These
              developments underscore the vast potential of multi-agent systems powered by foundation models to tackle
              increasingly complex challenges across diverse domains.
            </p>
            <p style="font-size: 20px; margin-bottom: 20px;">
              <strong>Our Workshop.</strong> This workshop seeks to bring together researchers from diverse disciplines
              to explore
              emerging topics in multi-agent systems powered by foundation models (LLMs, VLMs, and MMLMs). By
              complementing existing workshops focused on individual intelligence, it introduces a new perspective
              centered on advancing agent capabilities through multi-agent collaboration and their integration into
              human life. The primary objective of this workshop is to foster meaningful discussions and collaborations
              that address critical scientific questions surrounding the development and application of such systems.
            </p>
            <br>
            <!-- Previous Workshop: <a href="https://llmagents.github.io/" target="blank_">ICLR 2024 Workshop on LLM
                Agents</a> -->
            </p>
          </div>
          <!-- <div class="col-3">
            <p> <img src="assets/img/taming3.jpg" class="img-fluid" alt> <a></a> </p>
          </div> -->
          <div class="col-md-1">
          </div>
        </div>
      </div>

    </section><!-- End About Section -->

    <!-- ======= Topics Section ======= -->
    <section id="topics" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Call for papers</h2>
        </div>
        <div class="container">
          <div class="row">
            <p>This workshop aims to deepen understanding and offer innovative perspectives on the growth and
              collaboration of LLM/VLM-powered agents in multi-agent contexts. To foster an inclusive environment for
              discussion and debate, we welcome speakers and panelists from diverse backgrounds and expertise. Our
              lineup features distinguished researchers alongside emerging investigators who have made significant
              contributions to the field. Spotlight and poster sessions will highlight new ideas, key challenges, and
              retrospective insights related to the workshop‚Äôs themes. We strive for our participant selection to
              reflect the dynamic and diverse landscape of machine learning and AI.
              We will explore a range of topics in this workshop, including, but not limited to, the following areas:
            </p>
          </div>

          <!-- Panel for each topic -->
          <div class="row">
            <!-- Topic 1 -->
            <div class="col-md-4">
              <div class="topic-panel interactive-panel">
                <b>Multi-Agent Simulation:</b>
                <p>Simulations in society, games, psychology, economics, and politics.</p>
              </div>
            </div>

            <!-- Topic 2 -->
            <div class="col-md-4">
              <div class="topic-panel interactive-panel">
                <b>Multi-Agent Datasets and Benchmarks:</b>
                <p>The creation of datasets and benchmarks to inspire innovation and provide more effective evaluation
                  of the enhanced capabilities of multi-agent systems.</p>
              </div>
            </div>

            <!-- Topic 3 -->
            <div class="col-md-4">
              <div class="topic-panel interactive-panel">
                <b>Multi-Agent Orchestration and Efficiency:</b>
                <p>Multi-agent workflows, collaboration graphs, and communication protocols.</p>
              </div>
            </div>
          </div>

          <div class="row">
            <!-- Topic 4 -->
            <div class="col-md-3">
              <div class="topic-panel interactive-panel">
                <b>Human-Agent Collaboration:</b>
                <p>Exploring interactions and synergies between humans and agents to foster the development of more
                  trustworthy and socially friendly agents.</p>
              </div>
            </div>

            <!-- Topic 5 -->
            <div class="col-md-3">
              <div class="topic-panel interactive-panel">
                <b>Multi-Agent Applications:</b>
                <p>Math, software development, open question answering, web agents, os agents, mobile agents, embodied
                  agents for navigation, exploration, manipulation, autonomous systems, robotics, medical agents.</p>
              </div>
            </div>

            <!-- Topic 6 -->
            <div class="col-md-3">
              <div class="topic-panel interactive-panel">
                <b>Reinforcement Learning Methods for Multi-Agent Systems:</b>
                <p>Advance the multi-agent system from the interactive environment feedback.</p>
              </div>
            </div>

            <!-- Topic 7 -->
            <div class="col-md-3">
              <div class="topic-panel interactive-panel">
                <b>Symbolic Learning Methods for Multi-Agent Systems:</b>
                <p>Theoretical framework for multi-agent systems.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section><!-- End Topics Section -->


    <section id="SubmissionGuide" class="contact">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Submission Guide</h2>
        </div>
        <div class="row">
          <div class="col-md-1">
          </div>
          <div class="col-md-9">
            <p style="font-size: 20px; margin-bottom: 20px;">
              <span style="font-weight: bold;">Submission Platform:</span>
              Submit your papers here: <a
                href="https://openreview.net/group?id=ICML.cc/2025/Workshop/MAS&referrer=%5BHomepage%5D(%2F)"
                target="_blank">OpenReview Submission Site</a>
            </p>

            <p style="font-size: 20px; margin-bottom: 20px;">
              <span style="font-weight: bold;">Submission Requirements:</span><br>
              Use the official LaTeX template of ICML 2025 (<a
                href="https://media.icml.cc/Conferences/ICML2025/Styles/icml2025.zip" target="_blank">Style
                Files</a>)<br>
              Papers must be prepared and submitted as a single PDF: <strong>8 pages</strong> for the main paper, with
              unlimited pages for references and appendices (reviewers are not obliged to read the appendices)<br>
              All submissions must be <strong>anonymized</strong>, which will be reviewed in a
              <strong>double-blind</strong> manner
            </p>

            <p style="font-size: 20px; margin-bottom: 20px;">
              <span style="font-weight: bold;">Non-Archival Policy:</span>
              Submissions will <strong>not</strong> be indexed or have archival proceedings. We welcome NeurIPS 2025
              submissions.
            </p>

            <p style="font-size: 20px; margin-bottom: 20px;">
              <span style="font-weight: bold;">Best Paper Award:</span>
              Best paper award will be announced at the workshop.
            </p>

            <p style="font-size: 20px; margin-bottom: 20px;">
              <span style="font-weight: bold;">Key Dates</span><br>
              üìÑ Paper Submission Open: Apr 20, 2025, 11:59PM UTC-0 üê∞Happy Easter!<br>
              üìÑ Paper Submission Deadline: May 26, 2025, 11:59PM UTC-0<br>
              üìÑ Acceptance Notification: June 9, 2025, 11:59PM UTC-0<br>
              üìÑ Camera-Ready Deadline: June 30, 2025 11:59PM UTC-0<br>
              üìÑ Workshop Date: July 18, 2025, 11:59PM UTC-0
            </p>

          </div>
          <div class="col-md-1">
          </div>
        </div>
      </div>
    </section>
    <!-- End About Section -->

    <!-- ======= Schedule Section ======= -->
    <!-- ======= Workshop Schedule Section ======= -->
    <section id="schedule" class="schedule">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Workshop Schedule</h2>
        </div>

        <div class="row">
          <div class="col-lg-12">
            <div class="schedule-table">
              <table class="table">
                <thead>
                  <tr>
                    <th scope="col">Time</th>
                    <th scope="col">Session</th>
                    <th scope="col">Duration</th>
                    <th scope="col">Details</th>
                  </tr>
                </thead>
                <tbody>
                  <!-- Opening Remarks -->
                  <tr class="morning-session">
                    <td>9:00AM - 9:10AM</td>
                    <td>Opening Remarks</td>
                    <td>10 min</td>
                    <td>Welcome and Introduction to the Workshop</td>
                  </tr>
                  <!-- Invited Talk 1 -->
                  <tr class="morning-session">
                    <td>9:10AM - 9:30AM</td>
                    <td>Invited Talk 1</td>
                    <td>20 min</td>
                    <td>Talk1 - Yejin Choi</td>
                  </tr>
                  <!-- Invited Talk 2 -->
                  <tr class="morning-session">
                    <td>9:40AM - 10:00AM</td>
                    <td>Invited Talk 2</td>
                    <td>20 min</td>
                    <td>Talk2 - Natasha Jaques</td>
                  </tr>
                  <!-- Oral Presentations 1 -->
                  <tr class="morning-session">
                    <td>10:10AM - 10:40AM</td>
                    <td>Oral Presentations 1</td>
                    <td>30 min</td>
                    <td>15 min * 2</td>
                  </tr>
                  <!-- Poster Session & Coffee Socials 1 -->
                  <tr class="morning-session">
                    <td>10:45AM - 11:45AM</td>
                    <td>Poster Session & Coffee Socials 1</td>
                    <td>60 min</td>
                    <td>Poster Session & Coffee Socials1</td>
                  </tr>
                  <!-- Lunch Break -->
                  <tr>
                    <td>11:45AM - 1:00PM</td>
                    <td>Lunch Break</td>
                    <td>75 min</td>
                    <td>Time for lunch and informal discussions</td>
                  </tr>
                  <!-- Invited Talk 3 -->
                  <tr class="afternoon-session">
                    <td>1:00PM - 1:20PM</td>
                    <td>Invited Talk 3</td>
                    <td>20 min</td>
                    <td>Talk3 - Yilun Du</td>
                  </tr>
                  <!-- Invited Talk 4 -->
                  <tr class="afternoon-session">
                    <td>1:30PM - 1:40PM</td>
                    <td>Invited Talk 4</td>
                    <td>10 min</td>
                    <td>Talk4 - Diyi Yang</td>
                  </tr>
                  <!-- Oral Presentations 2 -->
                  <tr class="afternoon-session">
                    <td>1:50PM - 2:20PM</td>
                    <td>Oral Presentations 2</td>
                    <td>30 min</td>
                    <td>Oral Presentations2</td>
                  </tr>
                  <!-- Poster Session & Coffee Socials 2 -->
                  <tr class="afternoon-session">
                    <td>2:25PM - 3:25PM</td>
                    <td>Poster Session & Coffee Socials 2</td>
                    <td>60 min</td>
                    <td>Networking and refreshments</td>
                  </tr>
                  <!-- Invited Talk 5 -->
                  <tr class="afternoon-session">
                    <td>3:30PM - 3:50PM</td>
                    <td>Invited Talk 5</td>
                    <td>20 min</td>
                    <td>Talk5 - Mengdi Wang</td>
                  </tr>
                  <!-- Invited Talk 6 -->
                  <tr class="afternoon-session">
                    <td>4:00PM - 4:20PM</td>
                    <td>Invited Talk 6</td>
                    <td>20 min</td>
                    <td>Talk6 - Yulia Tsvetkov</td>
                  </tr>
                  <!-- Panel Discussion -->
                  <tr class="evening-session">
                    <td>4:30PM - 5:00PM</td>
                    <td>Panel Discussion</td>
                    <td>30 min</td>
                    <td>Interactive session with experts - Joon Sung Park, Mingchen Zhuge, Chen Qian, Mengyue Yang</td>
                  </tr>
                  <!-- Awards and Conclusive Remarks -->
                  <tr class="night-session">
                    <td>5:00PM - 5:15PM</td>
                    <td>Awards and Conclusive Remarks</td>
                    <td>15 min</td>
                    <td>Concluding the workshop and award announcements</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>

      </div>
    </section>

    <!-- End Workshop Schedule Section -->


    <section id="challenge" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Challenge</h2>
        </div>

        <div style="margin-bottom: 30px; line-height: 1.8;">

          <h4>Multi-Agent Embodied Intelligence Challenge @ ICML 2025</h4>
          <p>
            Advancing Embodied AI beyond single-agent settings, the Multi-Agent Embodied Intelligence Challenge, hosted
            at ICML 2025, invites the community to explore intelligent coordination in physically grounded environments.
            This challenge focuses on multi-agent manipulation via imitation learning, underpinned by compositional
            reasoning and scalable learning architectures.
          </p>

          <p>
            Hosted by the <strong><a href="https://mars-eai.github.io/MARS-System/" target="_blank"
                style="color: #007bff;">MARS-EAI</a></strong> initiative, which focuses on multi-agent embodied AI, this
            competition is built upon
            <strong><a href="https://iranqin.github.io/robofactory/" target="_blank"
                style="color: #007bff;">RoboFactory</a></strong> -- a novel simulation platform and benchmark tailored
            for collaborative robotic manipulation.
            RoboFactory introduces a constraint-driven framework where agent behavior is guided by logical, spatial, and
            temporal constraints. These compositional constraints are critical for enabling safe, efficient, and
            interpretable collaboration among embodied agents in shared spaces.
          </p>

          <p>
            Participants will train policies in a suite of challenging scenarios, ranging from dual-arm stacking to
            long-horizon cooperative tasks involving up to four agents.
            The core challenge lies in learning coordination policies that can handle uncertainty, leverage egocentric
            and global observations, and scale with the number of interacting agents.
          </p>

          <p><strong>Key highlights:</strong></p>
          <ul style="margin-left: 20px;">
            <li><strong>Compositional Imitation Learning</strong>: Participants must generate structured,
              constraint-aware behavior from demonstrations.</li>
            <li><strong>Benchmark Integration</strong>: Built atop ManiSkill3 and aligned with real-world application,
              RoboFactory enables rich, diverse task settings.</li>
            <li><strong>Evaluation Metrics</strong>: Performance will be assessed on task success, data efficiency,
              constraint compliance, and generalization under scene randomization.</li>
          </ul>

          <p>
            We invite researchers from robotics, imitation learning, multi-agent RL, and foundation model communities to
            participate.
            Top teams will be recognized at ICML 2025 and contribute to a shared vision of scalable, collaborative
            embodied intelligence.
          </p>
        </div>


        <!-- <div style="line-height: 2;">
          <strong>Related Homepage</strong>
          <br>
          <strong>MARS Homepage :</strong>
          <a href="https://mars-eai.github.io/MARS-System/" target="_blank" style="color: #007bff;">MARS</a>
          <br>
          <strong>RF Homepage :</strong>
          <a href="https://iranqin.github.io/robofactory/" target="_blank" style="color: #007bff;">RF</a>
        </div> -->

      </div>
    </section>


    <!-- ======= Speaker Section ======= -->
    <section id="speaker" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Speakers and panelists</h2>
        </div>

        <div class="row">
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/YejinChoi.jpeg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://yejinc.github.io/" target="_blank">Yejin Choi</a></h4>
                <strong>Senior Director at NVIDIA, Professor at Stanford</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Natasha.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://natashajaques.ai/" target="_blank">Natasha Jaques</a></h4>
                <strong>Assistant Professor, University of Washington, Senior Research Scientist at Google
                  DeepMind</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/yilun.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://yilundu.github.io/" target="_blank">Yilun Du</a></h4>
                <strong>Senior Research Scientist at Google Deepmind, Assistant Professor at Harvard</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Diyi_Yang.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://cs.stanford.edu/~diyiy/" target="_blank">Diyi Yang</a></h4>
                <strong>Assistant Professor at Stanford</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Mandy(Mengdi)Wang.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://ece.princeton.edu/people/mengdi-wang" target="_blank">Mengdi Wang</a></h4>
                <strong>Associate Professor at Princeton University</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Yulia.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://homes.cs.washington.edu/~yuliats/" target="_blank">Yulia Tsvetkov</a></h4>
                <strong>Associate Professor at University of Washington</strong>
              </div>
            </div>
          </div>

          <!-- <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Juergen Schmidhuber.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://people.idsia.ch/~juergen/" target="_blank">Juergen Schmidhuber</a></h4>
                <strong>Professor, King Abdullah University of Science and Technology</strong>
              </div>
            </div>
          </div> -->

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/joon.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.joonsungpark.com/" target="_blank">Joon Sung Park</a></h4>
                <strong>PhD Student at Stanford</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/mingchen.jpeg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://metauto.ai/" target="_blank">Mingchen Zhuge</a></h4>
                <strong>PhD Candidate at KAUST</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/qianchen.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://qianc62.github.io/" target="_blank">Chen Qian</a></h4>
                <strong>Associate Professor at Shanghai Jiao Tong University</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/MengyueYang.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://ymy4323460.github.io/" target="_blank">Mengyue Yang</a></h4>
                <strong>Assistant Professor, the University of Bristol.</strong>
              </div>
            </div>
          </div>
          
        </div>
    </section>

    <!-- ======= Panelist Section ======= -->
    <!-- <section id="panelist" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Panelist</h2>
        </div>

        <div class="row">
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/panelist/tao.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://taoyds.github.io/">Tao Yu</a></h4>
                <strong>Assistant Professor, The University of Hong Kong</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/panelist/roberta.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://rraileanu.github.io/">Roberta Raileanu</a></h4>
                <strong>Research Scientist, Meta GenAI</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/panelist/alexandre.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.alexdrouin.com/">Alexandre Drouin</a></h4>
                <strong>Staff Research Scientist, ServiceNow Research </strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/denny.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://dennyzhou.github.io/">Denny
                    Zhou</a></h4>
                <strong>Principal Scientist/Research Director, Google
                  DeepMind</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="https://www.nextcanada.com/wp-content/uploads/2019/09/graham-neubig.jpg" class="img-fluid"
                  alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.phontron.com/">Graham
                    Neubig</a></h4>
                <strong>Associate Professor, CMU LTI</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/luke.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a></h4>
                <strong>Professor, Allen School of Computer Science & Engineering, University of Washington</strong>
              </div>
            </div>
          </div>



        </div>
    </section> -->


    <!-- ======= CFP Section ======= -->
    <!-- <section id="cfp" class="cfp">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Call for Papers</h2>
        </div>

        <div class="row">
          <div class="col-lg-12">
            <div class="cfp-details">
              <h3>Important Dates:</h3>
              <ul>
                <li><strong>Submission Deadline:</strong> February
                  11th, 2024 (11:59 pm AoE)
                </li>
                <li><strong>Acceptance Notification:</strong>
                  <del>March 3rd, 2024</del> March 10th, 2024
                </li>

                <li><strong>Camera Ready Deadline:</strong> April
                  20th, 2024</li>
                <li><strong>Paper Availability on Website:</strong>
                  April
                  27th, 2024</li>
                <li><strong>Workshop Date:</strong> May 11th,
                  2024</li>
                <li><strong>Location:</strong> Vienna Exhibition &
                  Congress Center</li>
              </ul>

              <h3>Submission Tracks:</h3>

              <p>Consistent with the themes of the workshop, we
                invite
                contributions in the areas <a href="#topics">highlighted above</a>. However, we
                emphasize that the topics list is not exhaustive and
                welcome submissions in related areas. There is no
                need to specify your track on OpenReview. Our
                workshop will not accept work that has been
                previously published in other conferences on machine
                learning. Work that is presented at the main ICLR
                conference should not be submitted to us as well.

              </p>

              <ul>
                <li><strong>Research Paper Track:</strong> We
                  welcome a
                  variety of original research papers, including but
                  not limited to those
                  that propose new techniques, discussion-based
                  papers,
                  literature surveys, and position papers. Research
                  papers can have a <strong>maximum</strong> length
                  of up to 9
                  pages of content, plus unlimited pages
                  for references and appendix.</li>
                <li><strong>Demo Paper Track:</strong> We also
                  welcome
                  technical reports for the demo track, with a
                  <strong>maximum</strong>
                  of 9 pages (same as research papers). In addition
                  to the
                  paper, please provide a link to a video, website,
                  or
                  code
                  repository showcasing your demo.
                </li>
              </ul>

              <h3>Submission Guidelines:</h3>
              <ul>
                <li>üåê Submission Platform:
                  <ul>
                    <li>Submit your papers here: <strong><a
                          href="https://openreview.net/group?id=ICLR.cc/2024/Workshop/LLMAgents">Openreview
                          Submission
                          Site</a></strong></li>
                  </ul>
                </li>
                <li>üìÑ Paper Requirements:
                  <ul>
                    <li>Use the provided <a href="https://github.com/ICLR/Master-Template/raw/master/iclr2024.zip">LaTeX
                        template</a> for your
                      submission.</li>
                    <li>Papers should be anonymized and uploaded as
                      a
                      single PDF.</li>
                    <li>üìö References and Appendix: Reviewers are
                      not
                      obliged to read the appendix.</li>
                  </ul>
                </li>
                <li>üîç Non-Archival Policy:
                  <ul>
                    <li>Submissions will <strong>not be</strong>
                      indexed or have archival proceedings. We
                      welcome ICML 24 or ACL 24 submissions.</li>
                    <li>Accepted papers will be displayed on the
                      workshop website on <strong>27th April
                        2024</strong>. </li>
                  </ul>
                </li>
                <li>üîÑ Dual Submission Policy:
                  <ul>
                    <li>Submissions under review at other venues
                      will
                      be accepted, provided they do not breach any
                      dual-submission or anonymity policies of those
                      venues.</li>
                  </ul>
                </li>
                <li>üëÄ Review Process:
                  <ul>
                    <li>The review process is double-blind.</li>
                  </ul>
                </li>
                <li>üèÜ Best Paper Award:
                  <ul>
                    <li>The award for best paper will be
                      announced at the
                      workshop.</li>
                  </ul>
                </li>
              </ul>

            </div>
          </div>
        </div>

      </div>
    </section> -->
    <!-- End CFP Section -->

    <!-- ======= Accepted Papers Section ======= -->
    <!-- <section id="accepted-papers" class="accepted-papers">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Accepted Papers</h2>
        </div>
        <div class="accordion" id="papersAccordion">
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingOral">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                data-bs-target="#collapseOral" aria-expanded="true" aria-controls="collapseOral">
                Oral Presentations
              </button>
            </h2>
            <div id="collapseOral" class="accordion-collapse collapse" aria-labelledby="headingOral"
              data-bs-parent="#papersAccordion">
              <div class="accordion-body">
                <ul>
                  <li><strong> AutoGen: Enabling Next-Gen LLM
                      Applications via Multi-Agent
                      Conversation</strong>, <br>Qingyun Wu, Gagan
                    Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang
                    Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang,
                    Jiale Liu, Ahmed Hassan Awadallah, Ryen W White,
                    Doug Burger, Chi Wang</li>
                  <li><strong> Data-Copilot: Bridging Billions of
                      Data and Humans with Autonomous
                      Workflow</strong>, <br>Wenqi Zhang, Yongliang
                    Shen, Weiming Lu, Yueting Zhuang</li>
                  <li><strong> AutoAct: Automatic Agent Learning
                      from Scratch via Self-Planning</strong>,
                    <br>Shuofei Qiao, Ningyu Zhang, Runnan Fang,
                    Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor
                    Jiang, chengfei lv, Huajun Chen
                  </li>
                  <li><strong> Large Language Models can
                      Strategically Deceive their Users when Put
                      Under Pressure</strong>, <br>J√©r√©my Scheurer,
                    Mikita Balesni, Marius Hobbhahn</li>
                  <li><strong> Executable Code Actions Elicit Better
                      LLM Agents</strong>, <br>Xingyao Wang, Yangyi
                    Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao
                    Peng, Heng Ji</li>
                  <li><strong> Exploring Collaboration Mechanisms
                      for LLM Agents: A Social Psychology
                      View</strong>, <br>Jintian Zhang, Xin Xu,
                    Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin
                    Deng</li>
                </ul>
              </div>
            </div>
          </div>
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingPoster">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                data-bs-target="#collapsePoster" aria-expanded="false" aria-controls="collapsePoster">
                Poster Presentations
              </button>
            </h2>
            <div id="collapsePoster" class="accordion-collapse collapse" aria-labelledby="headingPoster"
              data-bs-parent="#papersAccordion">
              <div class="accordion-body">
                <ul>
                  <li><strong> Towards Unified Alignment Between
                      Agents, Humans, and Environment</strong>,
                    <br>Zonghan Yang, An Liu, Zijun Liu, Kaiming
                    Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang,
                    Qingyuan Hu, XinRui Chen, Zhenhe Zhang, Fuwen
                    Luo, Zhicheng Guo, Peng Li, Yang Liu
                  </li>
                  <li><strong> Self-Training Language Models in
                      Arithmetic Reasoning</strong>, <br>Marek
                    Kadlƒç√≠k, Michal ≈†tef√°nik, Ondrej Sotolar,
                    Vlastimil Martinek</li>
                  <li><strong> R2E: Turning any Github Repository
                      into a Programming Agent Test
                      Environment</strong>, <br>Naman Jain, Manish
                    Shetty, Tianjun Zhang, King Han, Koushik Sen,
                    Ion Stoica</li>
                  <li><strong> Lumos: Learning Agents with Unified
                      Data, Modular Design, and Open-Source
                      LLMs</strong>, <br>Da Yin, Faeze Brahman,
                    Abhilasha Ravichander, Khyathi Chandu, Kai-Wei
                    Chang, Yejin Choi, Bill Yuchen Lin</li>
                  <li><strong> LEAGUE++: EMPOWERING CONTINUAL ROBOT
                      LEARNING THROUGH GUIDED SKILL ACQUISITION WITH
                      LARGE LANGUAGE MODELS</strong>, <br>Zhaoyi Li,
                    Kelin Yu, Shuo Cheng, Danfei Xu</li>
                  <li><strong> WavCraft: Audio Editing and
                      Generation with Large Language
                      Models</strong>, <br>Jinhua Liang, Huan Zhang,
                    Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu,
                    Wenwu Wang, Mark D Plumbley, Huy Phan, Emmanouil
                    Benetos</li>
                  <li><strong> SAGE: Bridging Semantic and
                      Actionable Parts for Generalizable
                      Manipulation of Articulated Objects</strong>,
                    <br>Haoran Geng, Songlin Wei, Congyue Deng,
                    Bokui Shen, He Wang, Leonidas Guibas
                  </li>
                  <li><strong> Simulating Opinion Dynamics with
                      Networks of LLM-based Agents</strong>,
                    <br>Yun-Shiuan Chuang, Agam Goyal, Nikunj
                    Harlalka, Siddharth Suresh, Robert D. Hawkins,
                    Sijia Yang, Dhavan V. Shah, Junjie Hu, Timothy
                    T. Rogers
                  </li>
                  <li><strong> Agents: An Open-source Framework for
                      Autonomous Language Agents</strong>,
                    <br>Wangchunshu Zhou, Yuchen Eleanor Jiang, Long
                    Li, Jialong Wu, Tiannan Wang, Shuai Wang, Jiamin
                    Chen, Jintian Zhang, Jing Chen, Xiangru Tang,
                    Peng Cui, Ningyu Zhang, Huajun Chen, Mrinmaya
                    Sachan
                  </li>
                  <li><strong> A Human-Inspired Reading Agent with
                      Gist Memory of Very Long Contexts</strong>,
                    <br>Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta,
                    John Canny, Ian Fischer
                  </li>
                  <li><strong> The Agent Ohana: Designing Unified
                      Data and Training Pipeline for Effective Agent
                      Learning</strong>, <br>Jianguo Zhang, Tian
                    Lan, Rithesh R N, Zhiwei Liu, Weiran Yao, Juntao
                    Tan, Yihao Feng, Thai Quoc Hoang, Tulika Manoj
                    Awalgaonkar, Liangwei Yang, Shelby Heinecke,
                    Huan Wang, Juan Carlos Niebles, Silvio Savarese,
                    Caiming Xiong</li>
                  <li><strong> Can Large Language Models be Good
                      Path Planners? A Benchmark and Investigation
                      on Spatial-temporal Reasoning</strong>,
                    <br>Mohamed Aghzal, Erion Plaku, Ziyu Yao
                  </li>
                  <li><strong> FinMem: A Performance-Enhanced LLM
                      Trading Agent with Layered Memory and
                      Character Design</strong>, <br>Haohang Li,
                    Yangyang Yu, Zhi Chen, Yuechen Jiang, Yang Li,
                    Denghui Zhang, Rong Liu, Jordan W. Suchow,
                    Khaldoun Khashanah</li>
                  <li><strong> ArCHer: Training Language Model
                      Agents via Hierarchical Multi-Turn
                      RL</strong>, <br>Yifei Zhou, Andrea Zanette,
                    Jiayi Pan, Aviral Kumar, Sergey Levine</li>
                  <li><strong> Beyond A*: Better LLM planning via
                      Search Dynamics Bootstrapping</strong>,
                    <br>Lucas Lehnert, Sainbayar Sukhbaatar, Paul
                    McVay, Michael Rabbat, Yuandong Tian
                  </li>
                  <li><strong> A-CONECT: Designing AI-based
                      Conversational Chatbot for Early Dementia
                      Intervention</strong>, <br>Junyuan Hong,
                    Wenqing Zheng, Han Meng, Siqi Liang, Anqing
                    Chen, Hiroko H. Dodge, Jiayu Zhou, Zhangyang
                    Wang</li>
                  <li><strong> Agent Smith: A Single Image Can
                      Jailbreak One Million Multimodal LLM Agents
                      Exponentially Fast</strong>, <br>Xiangming Gu,
                    Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu,
                    Ye Wang, Jing Jiang, Min Lin</li>
                  <li><strong> Large Language Model Evaluation Via
                      Multi AI Agents: Preliminary results</strong>,
                    <br>Zeeshan Rasheed, Muhammad Waseem, Kari
                    Syst√§, Pekka Abrahamsson
                  </li>
                  <li><strong> Towards General Computer Control: A
                      Multimodal Agent for Red Dead Redemption II as
                      a Case Study</strong>, <br>Weihao Tan, Ziluo
                    Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng
                    Yue, Haochong Xia, Jiechuan Jiang, Longtao
                    Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun
                    Wang, B√∂rje F. Karlsson, Bo An, Zongqing Lu</li>
                  <li><strong> GPT-4V(ision) is a Generalist Web
                      Agent, if Grounded</strong>, <br>Boyuan Zheng,
                    Boyu Gou, Jihyung Kil, Huan Sun, Yu Su</li>
                  <li><strong> OpenAgents: An Open Platform for
                      Language Agents in the Wild</strong>,
                    <br>Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng
                    Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua,
                    Junning Zhao, Qian Liu, Che Liu, Zeyu Liu,
                    Yiheng Xu, Hongjin SU, Dongchan Shin, Caiming
                    Xiong, Tao Yu
                  </li>
                  <li><strong> OpenFMNav: Towards Open-Set Zero-Shot
                      Object Navigation via Vision-Language
                      Foundation Models</strong>, <br>Yuxuan Kuang,
                    Hai Lin, Meng Jiang</li>
                  <li><strong> TravelPlanner: A Benchmark for
                      Real-World Planning with Language
                      Agents</strong>, <br>Jian Xie, Kai Zhang,
                    Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong
                    Tian, Yanghua Xiao, Yu Su</li>
                  <li><strong> Empowering Autonomous Driving with
                      Large Language Models: A Safety
                      Perspective</strong>, <br>Yixuan Wang, Ruochen
                    Jiao, Simon Sinong Zhan, Chengtian Lang, Chao
                    Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu</li>
                  <li><strong> REX: Rapid Exploration and
                      eXploitation for AI agents</strong>,
                    <br>Rithesh R N, Shelby Heinecke, Juan Carlos
                    Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao
                    Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit,
                    Ran Xu, Phil L Mui, Huan Wang, Caiming Xiong,
                    Silvio Savarese
                  </li>
                  <li><strong> Towards Natural Language-Driven
                      Industrial Assembly Using Foundation
                      Models</strong>, <br>Omkar Joglekar, Shir
                    Kozlovsky, Tal Lancewicki, Vladimir Tchuiev,
                    Zohar Feldman, Dotan Di Castro</li>
                  <li><strong> Mobile-Agent: Autonomous Multi-Modal
                      Mobile Device Agent with Visual
                      Perception</strong>, <br>Junyang Wang, Haiyang
                    Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang,
                    Fei Huang, Jitao Sang</li>
                  <li><strong> Exposing Limitations of Language
                      Model Agents in Sequential-Task Compositions
                      on the Web</strong>, <br>Hiroki Furuta, Yutaka
                    Matsuo, Aleksandra Faust, Izzeddin Gur</li>
                  <li><strong> LLM Reasoners: New Evaluation,
                      Library, and Analysis of Step-by-Step
                      Reasoning with Large Language Models</strong>,
                    <br>Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu,
                    Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma,
                    Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting
                    Hu
                  </li>
                  <li><strong> R-Judge: Benchmarking Safety Risk
                      Awareness for LLM Agents</strong>, <br>Tongxin
                    Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang,
                    Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou,
                    Li Fangqi, Zhuosheng Zhang, Rui Wang, Gongshen
                    Liu</li>
                  <li><strong> LLF-Bench: Benchmark for Interactive
                      Learning from Language Feedback</strong>,
                    <br>Ching-An Cheng, Andrey Kolobov, Dipendra
                    Misra, Allen Nie, Adith Swaminathan
                  </li>
                  <li><strong> LLM-Deliberation: Evaluating LLMs
                      with Interactive Multi-Agent Negotiation
                      Game</strong>, <br>Sahar Abdelnabi, Amr Gomaa,
                    Sarath Sivaprasad, Lea Sch√∂nherr, Mario
                    Fritz</li>
                  <li><strong> Is it Possible to Edit Large Language
                      Models Robustly?</strong>, <br>Xinbei Ma,
                    Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, hai
                    zhao, lifeng Liu, Yulong Wang</li>
                  <li><strong> Agent Instructs Large Language Models
                      to be General Zero-Shot Reasoners</strong>,
                    <br>Nicholas Crispino, Kyle Montgomery, Fankun
                    Zeng, Dawn Song, Chenguang Wang
                  </li>
                  <li><strong> WorkArena: How Capable are Web Agents
                      at Solving Common Knowledge Work
                      Tasks?</strong>, <br>Alexandre Drouin, Maxime
                    Gasse, Massimo Caccia, Issam H. Laradji, Manuel
                    Del Verme, Tom Marty, David Vazquez, Nicolas
                    Chapados, Alexandre Lacoste</li>
                  <li><strong> Corex: Pushing the Boundaries of
                      Complex Reasoning through Multi-Model
                      Collaboration</strong>, <br>Qiushi Sun,
                    Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu,
                    Lingpeng Kong</li>
                  <li><strong> ProtAgents: Protein discovery via
                      large language model multi-agent
                      collaborations combining physics and machine
                      learning</strong>, <br>Alireza Ghafarollahi,
                    Markus Buehler</li>
                  <li><strong> Hierarchical Auto-Organizing System
                      for Open-Ended Multi-Agent
                      Navigation</strong>, <br>Zhonghan Zhao, Kewei
                    Chen, Dongxu Guo, Wenhao Chai, Tian Ye, Yanting
                    Zhang, Gaoang Wang</li>
                  <li><strong> EHRAgent: Code Empowers Large
                      Language Models for Few-shot Complex Tabular
                      Reasoning on Electronic Health
                      Records</strong>, <br>Wenqi Shi, Ran Xu,
                    Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu,
                    Yuanda Zhu, Joyce C. Ho, Carl Yang, May Dongmei
                    Wang</li>
                  <li><strong> Uncertainty of Thoughts:
                      Uncertainty-Aware Planning Enhances
                      Information Seeking in Large Language
                      Models</strong>, <br>Zhiyuan Hu, Chumin Liu,
                    Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan
                    Luu, Junxian He, Pang Wei Koh, Bryan Hooi</li>
                  <li><strong> TaskBench: Benchmarking Large
                      Language Models for Task Automation</strong>,
                    <br>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi
                    Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng
                    Li, Yueting Zhuang
                  </li>
                  <li><strong> SELF-IMAGINE: Effective Unimodal
                      Reasoning with Multimodal Models using
                      Self-Imagination</strong>, <br>Syeda Nahida
                    Akter, Aman Madaan, Sangwu Lee, Yiming Yang,
                    Eric Nyberg</li>
                  <li><strong> BioDiscoveryAgent: An AI Agent for
                      Designing Genetic Perturbation
                      Experiments</strong>, <br>Yusuf H Roohani,
                    Jian Vora, Qian Huang, Percy Liang, Jure
                    Leskovec</li>
                  <li><strong> MAGIC: INVESTIGATION OF LARGE
                      LANGUAGE MODEL POWERED MULTI-AGENT IN
                      COGNITION, ADAPTABILITY, RATIONALITY AND
                      COLLABORATION</strong>, <br>Lin Xu, Zhiyuan
                    Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt
                    Keutzer, See-Kiong Ng, Jiashi Feng</li>
                  <li><strong> Do LLM Agents Have Regret? A Case
                      Study in Online Learning and Games</strong>,
                    <br>Chanwoo Park, Xiangyu Liu, Asuman E.
                    Ozdaglar, Kaiqing Zhang
                  </li>
                  <li><strong> Prioritizing Safeguarding Over
                      Autonomy: Risks of LLM Agents for
                      Science</strong>, <br>Xiangru Tang, Qiao Jin,
                    Kunlun Zhu, Tongxin Yuan, Yichi Zhang,
                    Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian
                    Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu,
                    Mark Gerstein</li>
                  <li><strong> Expressing and Exploiting Parallelism
                      in Language Model Decoding</strong>, <br>Tian
                    Jin, Ellie Y Cheng, Michael Carbin</li>
                  <li><strong> Towards Self-Improving Language
                      Models for Code Generation</strong>,
                    <br>Micha√´l Defferrard, Corrado Rainone, David
                    W. Zhang, Blazej Manczak, Natasha Butt, Taco
                    Cohen
                  </li>
                  <li><strong> MathChat: Converse to Tackle
                      Challenging Math Problems with LLM
                      Agents</strong>, <br>Yiran Wu, Feiran Jia,
                    Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang,
                    Yin Tat Lee, Richard Peng, Qingyun Wu, Chi
                    Wang</li>
                  <li><strong> L3GO: Language Agents with
                      Chain-of-3D-Thoughts for Generating
                      Unconventional Objects</strong>, <br>Yutaro
                    Yamada, Khyathi Chandu, Bill Yuchen Lin, Jack
                    Hessel, Ilker Yildirim, Yejin Choi</li>
                  <li><strong> An Embodied Generalist Agent in 3D
                      World</strong>, <br>Jiangyong Huang, Silong
                    Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li,
                    Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia,
                    Siyuan Huang</li>
                  <li><strong> Agent-Pro: Learning to Evolve via
                      Policy-Level Reflection and
                      Optimization</strong>, <br>Wenqi Zhang, Ke
                    Tang, Hai Wu, Mengna Wang, Yongliang Shen,
                    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang,
                    Weiming Lu</li>
                  <li><strong> Recursive Speculative Decoding:
                      Accelerating LLM Inference via Sampling
                      Without Replacement</strong>, <br>Wonseok
                    Jeon, Mukul Gagrani, Raghavv Goel, Junyoung
                    Park, Mingu Lee, Christopher Lott</li>
                  <li><strong> VisualWebArena: Evaluating Multimodal
                      Agents on Realistic Visual Web Tasks</strong>,
                    <br>Jing Yu Koh, Robert Lo, Lawrence Jang,
                    Vikram Duvvur, Ming Chong Lim, Po-Yu Huang,
                    Graham Neubig, Shuyan Zhou, Ruslan
                    Salakhutdinov, Daniel Fried
                  </li>
                  <li><strong> HELPER-X: A Unified Instructable
                      Embodied Agent to Tackle Four Interactive
                      Vision-Language Domains with Memory-Augmented
                      Language Models</strong>, <br>Gabriel Herbert
                    Sarch, Sahil Somani, Raghav Kapoor, Michael J.
                    Tarr, Katerina Fragkiadaki</li>
                  <li><strong> Controlling Large Language
                      Model-based Agents for Large-Scale
                      Decision-Making: An Actor-Critic
                      Approach</strong>, <br>Bin Zhang, Hangyu Mao,
                    Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang,
                    Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, Lijuan
                    Li, Guoliang Fan</li>
                  <li><strong> Plan-Seq-Learn: Language Model Guided
                      RL for Solving Long Horizon Robotics
                      Tasks</strong>, <br>Murtaza Dalal, Tarun
                    Chiruvolu, Devendra Singh Chaplot, Ruslan
                    Salakhutdinov</li>
                  <li><strong> Adapting Uni-Modal Language Models
                      for Dense Multi-Modal Co-Reference Resolution
                      using Parameter Augmentation</strong>,
                    <br>Samuel Osebe, Prashan Wanigasekara, Thanh
                    Tran, Thomas Gueudre
                  </li>
                  <li><strong> Preference-Conditioned
                      Language-Guided Abstraction</strong>, <br>Andi
                    Peng, Andreea Bobu, Belinda Z. Li, Theodore
                    Sumers, Ilia Sucholutsky, Nishanth Kumar, Thomas
                    L. Griffiths, Julie Shah</li>
                  <li><strong> S-Agent: self-organizing agents in
                      open-ended environment</strong>, <br>Jiaqi
                    Chen, Yuxian Jiang, Jiachen Lu, Li Zhang</li>
                  <li><strong> Efficient Human-AI Coordination via
                      Preparatory Language-based
                      Convention</strong>, <br>Cong Guan, Lichao
                    Zhang, Chunpeng Fan, Yi-Chen Li, Feng Chen, Lihe
                    Li, Yunjia Tian, Lei Yuan, Yang Yu</li>
                  <li><strong> SeeClick: Harnessing GUI Grounding
                      for Advanced Visual GUI Agents</strong>,
                    <br>Kanzhi Cheng, Qiushi Sun, Yougang Chu,
                    Fangzhi Xu, Li YanTao, Jianbing Zhang, Zhiyong
                    Wu
                  </li>
                  <li><strong> The ART of LLM Refinement: Ask,
                      Refine, Trust</strong>, <br>Kumar
                    Shridhar</li>
                  <li><strong> SceneCraft: An LLM Agent for
                      Synthesizing 3D Scene as Blender
                      Code</strong>, <br>Ziniu Hu</li>
                  <li><strong> LangProp: A code optimization
                      framework using Large Language Models applied
                      to driving</strong>, <br>Shu Ishida, Gianluca
                    Corrado, George Fedoseev, Hudson Yeo, Lloyd
                    Russell, Jamie Shotton, Joao F. Henriques,
                    Anthony Hu</li>
                  <li><strong> FL-TAC: Enhanced Fine-Tuning in
                      Federated Learning via Low-Rank, Task-Specific
                      Adapter Clustering</strong>, <br>Siqi Ping,
                    Yuzhu Mao, Yang Liu, Xiao-Ping Zhang, Wenbo
                    Ding</li>
                  <li><strong> EcoAssistant: Using LLM Assistants
                      More Affordably and Accurately</strong>,
                    <br>Jieyu Zhang, Ranjay Krishna, Ahmed Hassan
                    Awadallah, Chi Wang
                  </li>
                  <li><strong> IntentGPT: Few-shot Intent Discovery
                      with Large Language Models</strong>, <br>Juan
                    A. Rodriguez, Nicholas Botzer, David Vazquez,
                    Christopher Pal, Marco Pedersoli, Issam H.
                    Laradji</li>
                  <li><strong> Language-guided Skill Learning with
                      Temporal Variational Inference</strong>,
                    <br>Haotian Fu, Pratyusha Sharma, Elias
                    Stengel-Eskin, George Konidaris, Nicolas Le
                    Roux, Marc-Alexandre C√¥t√©, Xingdi Yuan
                  </li>
                  <li><strong> Decision-Oriented Dialogue for
                      Human-AI Collaboration</strong>, <br>Jessy
                    Lin, Nicholas Tomlin, Jacob Andreas, Jason
                    Eisner</li>
                  <li><strong> Making Retrieval-Augmented Language
                      Models Robust to Irrelevant Context</strong>,
                    <br>Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan
                    Berant
                  </li>
                  <li><strong> MedAgents: Large Language Models as
                      Collaborators for Zero-shot Medical
                      Reasoning</strong>, <br>Xiangru Tang, Anni
                    Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao,
                    Xingyao Zhang, Arman Cohan, Mark Gerstein</li>
                  <li><strong> Collaborative LLM-Agents for Editable
                      Driving Scene Simulation</strong>, <br>Yuxi
                    Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing
                    Liu, Hao Zhao, Siheng Chen, Yanfeng Wang</li>
                  <li><strong> WebLINX: Real-World Website
                      Navigation with Multi-Turn Dialogue</strong>,
                    <br>Xing Han Lu, Zdenƒõk Kasner, Siva Reddy
                  </li>
                  <li><strong> The Wisdom of Partisan Crowds:
                      Comparing Collective Intelligence in Humans
                      and LLM-based Agents</strong>, <br>Yun-Shiuan
                    Chuang, Nikunj Harlalka, Siddharth Suresh, Agam
                    Goyal, Robert D. Hawkins, Sijia Yang, Dhavan V.
                    Shah, Junjie Hu, Timothy T. Rogers</li>
                  <li><strong> BOLAA: BENCHMARKING AND ORCHESTRATING
                      LLM AUTONOMOUS AGENTS</strong>, <br>Zhiwei
                    Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby
                    Heinecke, Rithesh R N, Yihao Feng, Zeyuan Chen,
                    Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil
                    L Mui, Huan Wang, Caiming Xiong, Silvio
                    Savarese</li>
                  <li><strong> Boosting Task Planning and Tool Usage
                      of Large Language Model-based Agents in
                      Real-world Systems</strong>, <br>Yilun Kong,
                    Jingqing Ruan, YiHong Chen, Bin Zhang, Tianpeng
                    Bao, shi shiwei, du guo qing, xiaoru hu, Hangyu
                    Mao, Ziyue Li, Xingyu Zeng, Rui Zhao, Xueqian
                    Wang</li>
                  <li><strong> Self-Alignment of Large Language
                      Models via Multi-Agent Social
                      Simulation</strong>, <br>Xianghe Pang, Shuo
                    Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng
                    Wang, Siheng Chen</li>
                  <li><strong> If LLM Is the Wizard, Then Code Is
                      the Wand: A Survey on How Code Empowers Large
                      Language Models to Serve as Intelligent
                      Agents</strong>, <br>Ke Yang, Jiateng Liu,
                    John Wu, Chaoqi Yang, Yi Fung, Sha Li, Zixuan
                    Huang, Xu Cao, Xingyao Wang, Heng Ji, ChengXiang
                    Zhai</li>
                  <li><strong> ReST meets ReAct: Self-Improvement
                      for Multi-Step Reasoning LLM Agent</strong>,
                    <br>Renat Aksitov, Sobhan Miryoosefi, Zonglin
                    Li, Daliang Li, Sheila Babayan, Kavya Kopparapu,
                    Zachary Fisher, Ruiqi Guo, Sushant Prakash,
                    Pranesh Srinivasan, Manzil Zaheer, Felix Yu,
                    Sanjiv Kumar
                  </li>
                  <li><strong> Are Machines Better at Slow Thinking?
                      Unveiling Human-Machine Inference Gaps in
                      Entailment Verification</strong>, <br>Soumya
                    Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang,
                    Xiang Ren</li>
                  <li><strong> Limitations of Agents Simulated by
                      Predictive Models</strong>, <br>Raymond
                    Douglas, Jacek Karwowski, Chan Bae, Andis
                    Draguns, Victoria Krakovna</li>
                  <li><strong> OS-Copilot: Towards Generalist
                      Computer Agents with
                      Self-Improvement</strong>, <br>Zhiyong Wu,
                    Chengcheng Han, Zichen Ding, Zhenmin Weng,
                    Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng
                    Kong</li>
                  <li><strong> EASYTOOL: Enhancing LLM-based Agents
                      with Concise Tool Instruction</strong>,
                    <br>Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu
                    Tan, Yongliang Shen, Kan Ren, Dongsheng Li,
                    Deqing Yang
                  </li>
                  <li><strong> FLASK: Fine-grained Language Model
                      Evaluation based on Alignment Skill
                      Sets</strong>, <br>Seonghyeon Ye, Doyoung Kim,
                    Sungdong Kim, Hyeonbin Hwang, Seungone Kim,
                    Yongrae Jo, James Thorne, Juho Kim, Minjoon
                    Seo</li>
                  <li><strong> Language Agent Tree Search Unifies
                      Reasoning Acting and Planning in Language
                      Models</strong>, <br>Andy Zhou, Kai Yan,
                    Michal Shlapentokh-Rothman, Haohan Wang,
                    Yu-Xiong Wang</li>
                  <li><strong> On the Road with GPT-4V(ision):
                      Explorations of Utilizing Visual-Language
                      Model as Autonomous Driving Agent</strong>,
                    <br>Licheng Wen, Xuemeng Yang, Daocheng Fu,
                    Xiaofeng Wang, Pinlong Cai, Xin Li, Tao MA,
                    Yingxuan Li, Linran XU, Dengke Shang, Zheng Zhu,
                    Shaoyan Sun, Yeqi BAI, Xinyu Cai, Min Dou,
                    Shuanglu Hu, Botian Shi, Yu Qiao
                  </li>
                  <li><strong> Bring Your Own KG: Self-Supervised
                      Program Synthesis for Zero-Shot KGQA</strong>,
                    <br>Dhruv Agarwal, Rajarshi Das, Sopan Khosla,
                    Rashmi Gangadharaiah
                  </li>
                  <li><strong> Open-TI: Open Traffic Intelligence
                      with Augmented Language Model</strong>,
                    <br>Longchao Da, Kuan-Ru Liou, Tiejin Chen,
                    Xuesong Zhou, Xiangyong Luo, Yezhou Yang, Hua
                    Wei
                  </li>
                  <li><strong> AgentBoard: An Analytical Evaluation
                      Board of Multi-turn LLM Agents</strong>,
                    <br>Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng
                    Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan,
                    Lingpeng Kong, Junxian He
                  </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Accepted Papers Section -->

    <!-- ======= Organization Section ======= -->
    <section id="org" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Workshop Organizers</h2>
          <!-- <h2>Organization</h2> -->
          <!-- <p>Workshop Organizers</p> -->
        </div>

        <!-- <div class="section-title" data-aos="zoom-out">
          <h3>Organizing Commitee</h3>
        </div> -->

        <div class="row">

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/zhenfei.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://yinzhenfei.github.io/" target="_blank">Zhenfei Yin</a></h4>
                <strong>PhD at USYD, Visiting Researcher at Oxford</strong>
                <!-- <br>
                <strong>Visiting Research Fellow, University of Oxford</strong> -->
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/huyue.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://phyllish.github.io/" target="_blank">Yue Hu</a></h4>
                <strong>Postdoctoral Fellow at University of Michigan</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/chensiheng.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://siheng-chen.github.io/" target="_blank">Siheng Chen</a></h4>
                <strong>Associate Professor at Shanghai Jiao Tong University</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/Bucher.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://bucherb.github.io/" target="_blank">Bernadette Bucher</a></h4>
                <strong>Assistant Professor at University of Michigan</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/ruiye_image.jpeg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://rui-ye.github.io" target="_blank">Rui Ye</a></h4>
                <strong>PhD Candidate at Shanghai Jiao Tong University</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/tangxiangru.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://xiangrutang.github.io/" target="_blank">Xiangru Tang</a></h4>
                <strong>Ph.D. Candidate at Yale University</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/ku.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://chahyon-ku.github.io/" target="_blank">Chahyon Ku</a></h4>
                <strong>PhD Student at University of Michigan</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/Ashton.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://k-ashton.github.io/" target="_blank">Katrina Ashton</a></h4>
                <strong>PhD Candidate at University of Pennsylvania</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/guo.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://ghli.org/" target="_blank">Guohao Li</a></h4>
                <strong>Founder of CAMEL-AI.org</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/zhangyilan.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="http://www.linkedin.com/in/yilan-zhang" target="_blank">Yilan Zhang</a></h4>
                <strong>Research Project Manager at Shanghai AI Laboratory</strong>
              </div>
            </div>
          </div>


          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/bailei.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="http://leibai.site/" target="_blank">Lei Bai</a></h4>
                <strong>Research Scientist at Shanghai AI Laboratory</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/Juan Carlos Niebles.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.niebles.net/" target="_blank">Juan Carlos Niebles</a></h4>
                <strong>Research Director at Salesforce AI Research, Adjunct Professor at Stanford</strong>
              </div>
            </div>
          </div>

          <!-- <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/dawn-berkeley.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://dawnsong.io/" target="_blank">Dawn Song</a></h4>
                <strong>Professor at University of California, Berkeley</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/PhilipTorr.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="http://www.robots.ox.ac.uk/~phst/" target="_blank">Philip Torr</a></h4>
                <strong>Professor at University of Oxford</strong>
              </div>
            </div>
          </div> -->

        </div>
      </div>

    </section><!-- End Organization Section -->

    <!-- ======= Reviewers Section ======= -->
    <!-- <section id="reviewers" class="reviewers">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Program Committee</h2>
        </div>
        <div class="accordion" id="reviewersAccordion">
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingReviewer">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                data-bs-target="#collapseReviewer" aria-expanded="true" aria-controls="collapseReviewer">
                Reviewers
              </button>
            </h2>
            <div id="collapseReviewer" class="accordion-collapse collapse" aria-labelledby="headingReviewer"
              data-bs-parent="#papersAccordion">
              <div class="accordion-body">

                <div class="row">
                  <div class="col-lg-12">
                    <ul class="list-unstyled">
                      <li>Yuelyu Ji, <it>University of
                          Pittsburgh</it>
                      </li>
                      <li>Hangyu Mao, <it>Sensetime
                          Research</it>
                      </li>
                      <li>Boyuan Zheng, <it>Ohio State University,
                          Columbus</it>
                      </li>
                      <li>Siyu Yuan, <it>Fudan University</it>
                      </li>
                      <li>Xin Cong, <it>Tsinghua University,
                          Tsinghua
                          University</it>
                      </li>
                      <li>Markus Buehler, <it>Massachusetts
                          Institute of
                          Technology</it>
                      </li>
                      <li>Lin Xu, <it>National University of
                          Singapore</it>
                      </li>
                      <li>Chenfei Yuan, <it>Department of Computer
                          Science
                          and Technology, Tsinghua
                          University</it>
                      </li>
                      <li>Haochen Vector Zhao, <it>Peking
                          University</it>
                      </li>
                      <li>Feiran Jia, <it>Pennsylvania State
                          University</it>
                      </li>
                      <li>Yao Yao, <it>Shanghai Jiaotong
                          University</it>
                      </li>
                      <li>Zhang Ruichen, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Mathieu Ravaut, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Zirui Zhao, <it>national university of
                          singaore,
                          National University of Singapore</it>
                      </li>
                      <li>Jialong Wu, <it>Southeast
                          University</it>
                      </li>
                      <li>Rithesh R N, <it>SalesForce.com</it>
                      </li>
                      <li>Juntao Tan, <it>Rutgers
                          University</it>
                      </li>
                      <li>Ting Chen, <it>University of Electronic
                          Science
                          and Technology of China</it>
                      </li>
                      <li>Yun-Shiuan Chuang, <it>University of
                          Wisconsin -
                          Madison</it>
                      </li>
                      <li>Jiageng Mao, <it>University of Southern
                          California</it>
                      </li>
                      <li>Yongliang Shen, <it>Microsoft</it>
                      </li>
                      <li>Zhiruo Wang, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Jiuzhou Han, <it>Monash
                          University</it>
                      </li>
                      <li>Kaixin Ma, <it>Tencent AI Lab</it>
                      </li>
                      <li>Hao Peng, <it>Department of Computer
                          Science,
                          University of Illinois
                          Urbana-Champaign</it>
                      </li>
                      <li>Jian Guan, <it>Tsinghua University,
                          Tsinghua
                          University</it>
                      </li>
                      <li>Shaoguang Mao, <it>Microsoft</it>
                      </li>
                      <li>Olivia Watkins, <it>University of
                          California
                          Berkeley</it>
                      </li>
                      <li>Jiateng Liu, <it>Department of Computer
                          Science</it>
                      </li>
                      <li>Qian Huang, <it>Google</it>
                      </li>
                      <li>Haozhe Zhao, <it>Peking
                          University</it>
                      </li>
                      <li>Yecheng Jason Ma, <it>University of
                          Pennsylvania</it>
                      </li>
                      <li>Zhenran Xu, <it>Harbin Institute of
                          Technology,
                          Shenzhen</it>
                      </li>
                      <li>Zhongshen Zeng, <it>Department of Computer
                          Science
                          and Engineering, The Chinese University of
                          Hong
                          Kong</it>
                      </li>
                      <li>Kuang-Huei Lee, <it>Google</it>
                      </li>
                      <li>Chunyuan Deng, <it>Georgia Institute of
                          Technology</it>
                      </li>
                      <li>Meghana Moorthy Bhat, <it>Salesforce
                          Research</it>
                      </li>
                      <li>Tianjun Zhang, <it>University of
                          California
                          Berkeley</it>
                      </li>
                      <li>Jiangyong Huang, <it>Peking
                          University</it>
                      </li>
                      <li>Wenshan Wu, <it>Microsoft</it>
                      </li>
                      <li>Kimin Lee, <it>Korea Advanced Institute of
                          Science
                          & Technology</it>
                      </li>
                      <li>Daquan Zhou, <it>Bytedance</it>
                      </li>
                      <li>Haoqi Yuan, <it>Peking
                          University</it>
                      </li>
                      <li>Osbert Bastani, <it>University of
                          Pennsylvania</it>
                      </li>
                      <li>Shuyan Zhou, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Agam Goyal, <it>University of Wisconsin -
                          Madison</it>
                      </li>
                      <li>Gang Qiao, <it>Siemens
                          Healthineers</it>
                      </li>
                      <li>Xun Wang, <it>Microsoft</it>
                      </li>
                      <li>Sahitya Potluri, <it>Google</it>
                      </li>
                      <li>Xingyao Wang, <it>Department of Computer
                          Science,
                          University of Illinois
                          Urbana-Champaign</it>
                      </li>
                      <li>Wenyue Hua, <it>Rutgers University, New
                          Brunswick</it>
                      </li>
                      <li>Younggyo Seo, <it>Dyson</it>
                      </li>
                      <li>Zhangcheng Qiang, <it>Australian National
                          University</it>
                      </li>
                      <li>Boyu Gou, <it>Ohio State University,
                          Columbus</it>
                      </li>
                      <li>Jian Xie, <it>Fudan University</it>
                      </li>
                      <li>Ziniu Hu, <it>California Institute of
                          Technology</it>
                      </li>
                      <li>Yichi Zhang, <it>Peking
                          University</it>
                      </li>
                      <li>Fangkai Jiao, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Yangyi Chen, <it>School of Computer
                          Science,
                          University of Illinois at
                          Urbana-Champaign</it>
                      </li>
                      <li>Ravi Pandya, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Zelong Li, <it>Rutgers University, New
                          Brunswick</it>
                      </li>
                      <li>Jiayuan Mao, <it>Massachusetts Institute
                          of
                          Technology</it>
                      </li>
                      <li>Bohan Lyu, <it>Tsinghua
                          University</it>
                      </li>
                      <li>Senbao Shi, <it>Harbin Institute of
                          Technology</it>
                      </li>
                      <li>Kaitao Song, <it>Microsoft</it>
                      </li>
                      <li>Nikunj Harlalka, <it>University of
                          Wisconsin -
                          Madison</it>
                      </li>
                      <li>Zhihan Liu, <it>Northwestern
                          University</it>
                      </li>
                      <li>Haochun Wang, <it>Harbin Institute of
                          Technology</it>
                      </li>
                      <li>Chi Zhang, <it>Tencent </it>
                      </li>
                      <li>Chang Gao, <it>The Chinese University of
                          Hong
                          Kong</it>
                      </li>
                      <li>Kun Shao, <it>Huawei Noah's Ark
                          Lab</it>
                      </li>
                      <li>Lanqing Li, <it>Zhejiang Lab</it>
                      </li>
                      <li>Ziyuan Qin, <it>Case Western Reserve
                          University</it>
                      </li>
                      <li>Chengjie Zheng, <it>University of
                          Massachusetts
                          Boston</it>
                      </li>
                      <li>Bharat Prakash, <it>University of
                          Maryland,
                          Baltimore County</it>
                      </li>
                      <li>Yanjun Shao, <it>Fudan
                          University</it>
                      </li>
                      <li>Amrita Saha, <it>SalesForce.com</it>
                      </li>
                      <li>Ke Yang, <it>Department of Computer
                          Science</it>
                      </li>
                      <li>Zhao Xu, <it>Hong Kong University of
                          Science and
                          Technology</it>
                      </li>
                      <li>Ruochen Zhao, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Chaoqi Yang, <it>University of Illinois
                          Urbana
                          Champaign</it>
                      </li>
                      <li>Hao Wang, <it>Google</it>
                      </li>
                      <li>Yangyang Yu, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Shuofei Qiao, <it>Zhejiang
                          University</it>
                      </li>
                      <li>Hailin Chen, <it>National Technological
                          University</it>
                      </li>
                      <li>Yuan Yao, <it>Nanjing University</it>
                      </li>
                      <li>Lei Liu, <it>The Chinese University of
                          Hong Kong,
                          Shenzhen</it>
                      </li>
                      <li>Yuechen Jiang, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Pengguang Chen, <it>SmartMore</it>
                      </li>
                      <li>Chuan Xiao, <it>Osaka University</it>
                      </li>
                      <li>Sha Li, <it>University of Illinois, Urbana
                          Champaign</it>
                      </li>
                      <li>Wenqi Zhang, <it>Zhejiang
                          University</it>
                      </li>
                      <li>Yilun Zhao, <it>Yale University</it>
                      </li>
                      <li>Kaikai An, <it>Peking University</it>
                      </li>
                      <li>Yunhao Yang, <it>University of Texas at
                          Austin</it>
                      </li>
                      <li>Haohang Li, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Jianghao Zhang, <it>University of Michigan
                          - Ann
                          Arbor</it>
                      </li>
                      <li>Shruti Singh, <it>IIT
                          Gandhinagar</it>
                      </li>
                      <li>Zhi Chen, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>He Zhu, <it>Rutgers University</it>
                      </li>
                      <li>Allen Nie, <it>Stanford
                          University</it>
                      </li>
                      <li>Shuzheng Si, <it>Peking
                          University</it>
                      </li>
                      <li>Muhammad Waseem, <it>University of
                          Jyv√§skyl√§</it>
                      </li>
                      <li>Jing Yu Koh, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Kunlun Zhu, <it>Universit√© de
                          Montr√©al</it>
                      </li>
                      <li>Chengwei Qin, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Zengqing Wu, <it>Kyoto
                          University</it>
                      </li>
                      <li>Vernon Bumgardner, <it>University of
                          Kentucky</it>
                      </li>
                      <li>Chenyang Zhao, <it>Zhejiang Lab</it>
                      </li>
                      <li>Rong Liu, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Sihao Hu, <it>Georgia Institute of
                          Technology</it>
                      </li>
                      <li>Srijan Bansal, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Da Yin, <it>University of California, Los
                          Angeles</it>
                      </li>
                      <li>Hung Le, <it>Salesforce Research</it>
                      </li>
                      <li>Enxhell Luzhnica, <it>Google</it>
                      </li>
                      <li>Michelle D Zhao, <it>CMU, Carnegie Mellon
                          University</it>
                      </li>
                      <li>Yunfan Jiang, <it>Stanford
                          University</it>
                      </li>
                      <li>Hongyang Du, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Jason Phang, <it>New York
                          University</it>
                      </li>
                      <li>Xingxuan Li, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Mingqi Gao, <it>Peking
                          University</it>
                      </li>
                      <li>Xiao Han, <it>Peking University</it>
                      </li>
                      <li>Haojie Pan, <it>Department of Computer
                          Science and
                          Engineering, Hong Kong University of
                          Science and
                          Technology</it>
                      </li>
                      <li>Pekka Abrahamsson, <it>Tampere
                          University</it>
                      </li>
                      <li>Haibin Huang, <it>Kuaishou
                          Technology</it>
                      </li>
                      <li>Yiming Zhang, <it>Tokyo Institute of
                          Technology,
                          Tokyo Institute of Technology</it>
                      </li>
                      <li>Baotian Hu, <it>Harbin Institute of
                          Technology,
                          Shenzhen</it>
                      </li>
                      <li>Yang Yuan, <it>Tsinghua University,
                          Tsinghua
                          University</it>
                      </li>
                      <li>Yixin Zhang, <it>Kyoto University, Kyoto
                          University</it>
                      </li>
                      <li>Riccardo Cantini, <it>University of
                          Calabria</it>
                      </li>
                      <li>Tiankai Hang, <it>Southeast
                          University</it>
                      </li>
                      <li>Gongshen Liu, <it>Shanghai Jiao Tong
                          University</it>
                      </li>
                      <li>Yuzhou Du, <it>Northwestern
                          University</it>
                      </li>
                      <li>Xiaocheng Lu, <it>Hong Kong Polytechnic
                          University</it>
                      </li>
                      <li>Sarang Gupta, <it>Asana</it>
                      </li>
                      <li>Inderjeet Jayakumar Nair, <it>University
                          of
                          Michigan - Ann Arbor</it>
                      </li>
                      <li>Gabrielle Kaili-May Liu, <it>Department of
                          Computer Science, Yale
                          University</it>
                      </li>
                      <li>Shuyuan Zheng, <it>Osaka
                          University</it>
                      </li>
                      <li>Run Peng, <it>University of Michigan - Ann
                          Arbor</it>
                      </li>
                      <li>Mira Moukheiber, <it>Massachusetts
                          Institute of
                          Technology</it>
                      </li>
                      <li>John Wu, <it>University of Illinois at
                          Urbana-Champaign</it>
                      </li>
                      <li>Bin Liu, <it>Zhejiang Lab</it>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Reviewers Section -->




    <!-- ======= Contact Section ======= -->
    <!-- <section id="award" class="award">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Award</h2>
        </div>
        <div>
          <h5>The Best Paper Award:
            <br><br>
            AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

            <br><br>
            Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun
            Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang

          </h5>

        </div>

      </div>

    </section> -->
    <!-- End Contact Section -->




    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Contact us</h2>
        </div>
        <div>
          <h5>Email us at <a href="mailto:masworks2025@gmail.com">masworks2025@gmail.com</a>
            | <a href="mailto:zhenfei.yin@sydney.edu.au">zhenfei.yin@sydney.edu.au</a> | <a
              href="mailto:huyu@umich.edu">huyu@umich.edu</a>
          </h5>
        </div>

        <div>
          <h5>This is also an event hosted by<a href="https://github.com/MASWorks" target="_blank"> [MASWorks Community] </a>
          </h5>
        </div>

      </div>

    </section><!-- End Contact Section -->

    <!-- ======= Contact Section ======= -->
    <section id="sponsors" class="sponsors">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Sponsors</h2>
        </div>

        <div style="display: flex; justify-content: center; align-items: center; gap: 40px; flex-wrap: wrap;">

          <!-- CAMEL AI -->
          <a href="https://www.camel-ai.org/" target="_blank" style="display: flex; align-items: center;">
            <svg viewBox="0 0 119 24" fill="none" xmlns="http://www.w3.org/2000/svg"
              style="max-height: 80px; height: 80px; width: auto; display: inline-block;">
              <path
                d="M4.60488 4.40261L4.03345 5.19382C4.38719 5.46666 5.01304 5.71221 5.63889 5.54851C6.01984 5.84863 6.60034 5.80315 6.89059 5.79406C6.84524 6.26697 6.78175 7.79121 6.89059 10.1048C6.99943 12.4185 8.67744 13.9973 9.50283 14.4975L8.79535 17.6624L9.50283 20.7999H11.0811L10.4824 17.6624L11.0811 14.4975L12.4416 20.7999H14.0471L12.7137 14.4975H16.1423L14.809 20.7999H16.4416L17.7749 14.4975L18.4008 17.6624L17.7749 20.7999H19.4076L20.0334 17.6624L19.3804 14.4975C19.5164 14.1428 19.5888 14.0043 19.6797 13.6244C20.1695 11.5781 19.2443 9.92295 18.7001 9.31362L17.394 7.97674C16.8226 7.40378 15.843 7.3765 15.2171 7.89488C14.757 8.27598 14.4965 8.29701 14.0471 7.89488C13.8566 7.73118 13.3857 7.17864 12.7681 6.66713C12.1423 6.14874 11.0539 6.36701 10.6457 6.80355L8.79535 8.65882V3.69324L9.15345 3.19995C7.97431 3.43641 5.57358 3.81874 5.31236 3.88422C5.05113 3.9497 4.73186 4.25709 4.60488 4.40261Z"
                fill="#4215CC"></path>
              <path d="M6.66968 4.4114L7.05063 4.04456L7.43158 4.4114L7.05063 4.80646L6.66968 4.4114Z" fill="white">
              </path>
              <path
                d="M24.3 14.2476C24.3 12.5326 24.8426 11.1238 25.9276 10.0213C26.4351 9.5021 27.0243 9.08793 27.6951 8.77876C28.8326 8.25959 30.1539 8 31.6589 8C32.4406 8 33.1318 8.06125 33.7327 8.18375C34.3335 8.30042 35.0423 8.52209 35.859 8.84876L35.964 9.04126C35.8415 9.56044 35.7044 10.4763 35.5527 11.7888H34.8527L34.7652 10.2488C34.3393 9.84627 33.9514 9.56627 33.6014 9.40877C33.0064 9.14043 32.3327 9.00626 31.5801 9.00626C29.9643 9.00626 28.7422 9.57502 27.9138 10.7125C27.2955 11.5584 26.9863 12.6521 26.9863 13.9938C26.9863 15.5572 27.3888 16.8318 28.1939 17.8176C29.1622 19.0076 30.501 19.6027 32.2102 19.6027C33.3768 19.6027 34.5406 19.2964 35.7014 18.6839L35.9115 18.9026L35.5177 19.7777C35.016 20.0518 34.596 20.2443 34.2577 20.3552C33.4293 20.6293 32.5047 20.7664 31.4839 20.7664C29.9789 20.7664 28.678 20.4922 27.5813 19.9439C26.8288 19.5706 26.1813 19.0514 25.6388 18.3864C24.7463 17.2897 24.3 15.9101 24.3 14.2476Z"
                fill="#4215CC"></path>
              <path
                d="M37.0323 20.7664V19.8477L37.5661 19.7952C37.9569 19.7602 38.2923 19.4014 38.5723 18.7189L41.8186 10.87L42.8424 8.16625H44.1111L48.7487 19.0076C48.9529 19.4743 49.2825 19.7368 49.7375 19.7952L50.1662 19.8477V20.7664C48.6029 20.7314 47.6719 20.7664 47.351 20.7664C46.9952 20.7664 46.0741 20.7314 44.6099 20.7664V19.8477L45.5637 19.7952C45.6628 19.7893 45.7445 19.7397 45.8087 19.6464C45.8787 19.5531 45.9137 19.4422 45.9137 19.3139C45.9137 19.1506 45.8787 18.9843 45.8087 18.8151L45.0037 16.7676H40.5061L39.7536 18.7714C39.7011 18.9231 39.6748 19.0718 39.6748 19.2176C39.6748 19.3751 39.7215 19.5093 39.8148 19.6202C39.914 19.731 40.0394 19.7893 40.1911 19.7952L41.0836 19.8477V20.7676C40.6286 20.7559 40.2611 20.7727 40.0336 20.7669C39.5494 20.7494 39.2189 20.7664 39.038 20.7664H37.0323ZM40.9436 15.7614H44.5924L42.7286 11.2113L40.9436 15.7614Z"
                fill="#4215CC"></path>
              <path
                d="M51.1821 20.7664V19.8477L51.8471 19.7952C52.1971 19.766 52.4217 19.6931 52.5208 19.5764C52.6433 19.4364 52.7046 18.9231 52.7046 18.0364V10.73C52.7046 9.93086 52.6608 9.44669 52.5733 9.27752C52.4917 9.10251 52.2496 9.00043 51.8471 8.97126L51.1821 8.91876V8.25375L52.0921 8.28C52.6871 8.2975 53.0283 8.30625 53.1158 8.30625H53.3346L55.4871 8.25375C56.1638 9.72377 56.6101 10.6863 56.8259 11.1413L59.6609 17.0564L62.9335 10.275C63.0968 9.95419 63.3914 9.28043 63.8172 8.25375C65.0597 8.28875 65.7452 8.30625 65.8735 8.30625C66.0077 8.30625 66.7019 8.28875 67.956 8.25375V8.91876L67.291 8.97126C66.941 9.00043 66.7135 9.07335 66.6085 9.19002C66.4918 9.33002 66.4335 9.84336 66.4335 10.73V18.0364C66.4335 18.8414 66.4714 19.3197 66.5473 19.4714C66.6231 19.6172 66.871 19.7077 67.291 19.7427L67.956 19.7952V20.7664C66.6202 20.7314 65.7003 20.7664 65.1752 20.7664C64.6444 20.7664 63.7064 20.7314 62.3822 20.7664V19.7952L63.0472 19.7427C63.4206 19.7135 63.651 19.6493 63.7385 19.5502C63.8493 19.4276 63.9047 18.9231 63.9047 18.0364V10.5813L60.2122 18.2726C59.7455 19.241 59.3868 20.0285 59.1359 20.6352H58.5322C58.2172 19.8885 58.0013 19.3926 57.8847 19.1476L53.7633 10.5638V18.0364C53.7633 18.8356 53.8042 19.3226 53.8859 19.4977C53.9675 19.6668 54.2125 19.766 54.6209 19.7952L55.2859 19.8477V20.7664C54.224 20.7664 53.5668 20.7664 53.2284 20.7664C52.8843 20.7664 52.2262 20.7314 51.1821 20.7664Z"
                fill="#4215CC"></path>
              <path
                d="M69.3315 8.91876V8.25375C70.9298 8.28875 72.0557 8.30625 72.709 8.30625C73.2574 8.30625 74.0332 8.29459 75.0365 8.27125C75.8941 8.25375 76.5386 8.245 76.9703 8.245C77.5653 8.245 78.1312 8.26542 78.6678 8.30625L78.7728 8.42876L78.5278 10.9838H77.8803L77.7403 9.63627C77.7287 9.54294 77.5537 9.46419 77.2153 9.40002C76.8828 9.33002 76.4774 9.29502 75.9991 9.29502C75.0715 9.29502 74.1995 9.31252 73.3828 9.34752V13.6526C73.6861 13.6759 74.0857 13.6876 74.5815 13.6876H75.7628C76.1478 13.6876 76.4103 13.6613 76.5503 13.6088C76.6903 13.5563 76.7691 13.4572 76.7866 13.3113L76.8916 12.2876H77.6091C77.6032 12.5734 77.5974 12.7892 77.5916 12.9351C77.5682 13.5242 77.5566 13.883 77.5566 14.0113C77.5566 14.1105 77.5566 14.1747 77.5566 14.2038L77.6091 16.1376H76.8916L76.7866 15.2451C76.7574 15.0059 76.6641 14.8368 76.5066 14.7376C76.3491 14.6326 76.1011 14.5801 75.7628 14.5801H74.5815C74.2257 14.5801 73.8261 14.5976 73.3828 14.6326V19.4189C74.2578 19.4656 75.0424 19.4889 75.7366 19.4889C76.6349 19.4889 77.3262 19.3985 77.8103 19.2176C77.8395 19.206 77.857 19.1914 77.8628 19.1739L78.2741 17.4851H78.9741L78.7728 20.3202L78.7221 20.7751C77.8879 20.8276 76.9795 20.781 76.1336 20.781C75.2469 20.781 74.2848 20.8087 73.2465 20.7737C72.7623 20.7562 72.3816 20.7664 72.1016 20.7664C71.6991 20.7664 70.994 20.7314 69.9265 20.7664V20.1014L70.4952 19.7777C70.7344 19.6435 70.854 19.3022 70.854 18.7539V10.73C70.854 9.93086 70.8102 9.44669 70.7227 9.27752C70.6411 9.10835 70.399 9.00626 69.9965 8.97126L69.3315 8.91876Z"
                fill="#4215CC"></path>
              <path
                d="M80.2866 8.97126V8.25375C81.7275 8.28875 82.655 8.30625 83.0692 8.30625C83.4775 8.30625 84.4079 8.28875 85.8604 8.25375V8.97126L85.1954 9.02376C84.8163 9.05293 84.5829 9.1171 84.4954 9.21627C84.3904 9.3446 84.3379 9.84919 84.3379 10.73V19.4189C85.1838 19.4656 85.9684 19.4889 86.6917 19.4889C87.6075 19.4889 88.2988 19.3985 88.7655 19.2176C88.7946 19.206 88.8121 19.1914 88.818 19.1739L89.2292 17.3451H89.9L89.728 20.3202L89.6236 20.7737C88.7894 20.8262 87.9307 20.7737 87.0849 20.7737C86.1982 20.7737 85.2858 20.8087 84.2475 20.7737C83.7633 20.7562 83.3328 20.7737 83.0528 20.7737C82.6503 20.7737 81.9491 20.7387 80.8816 20.7737V20.1014L81.4504 19.7777C81.6896 19.6435 81.8091 19.3051 81.8091 18.7626V10.73C81.8091 9.93086 81.7712 9.45544 81.6954 9.30377C81.6195 9.1521 81.3716 9.05876 80.9516 9.02376L80.2866 8.97126Z"
                fill="#4215CC"></path>
              <path
                d="M90.4335 14.9033C90.4825 14.7572 90.5351 14.6043 90.5911 14.4443C90.6857 14.1836 90.7558 13.9733 90.8014 13.8134H94.4989L94.5935 13.9229C94.4253 14.3122 94.3114 14.6668 94.2519 14.9867H90.5176L90.4335 14.9033Z"
                fill="#4215CC"></path>
              <path
                d="M95.1268 20.7987V19.8727L95.6606 19.8202C96.0514 19.7852 96.3868 19.4264 96.6669 18.7439L99.9131 10.8951L100.94 8.00001H102.22L106.843 19.0327C107.047 19.4994 107.377 19.7619 107.832 19.8202L108.261 19.8727V20.7987C106.7 20.7987 105.749 20.7987 105.428 20.7987C105.072 20.7987 104.14 20.7987 102.704 20.7987V19.8727L103.658 19.8202C103.757 19.8144 103.839 19.7648 103.903 19.6714C103.973 19.5781 104.008 19.4673 104.008 19.3389C104.008 19.1756 103.973 19.0094 103.903 18.8402L103.098 16.7927H98.6006L97.8481 18.7964C97.7956 18.9481 97.7694 19.0969 97.7694 19.2427C97.7694 19.4002 97.816 19.5344 97.9094 19.6452C98.0085 19.756 98.134 19.8144 98.2856 19.8202L99.1781 19.8727V20.7987C98.7231 20.7871 98.3879 20.8046 98.1604 20.7987C97.6602 20.7987 97.2959 20.7987 97.1151 20.7987H95.1268ZM99.0381 15.7864H102.687L100.823 11.2363L99.0381 15.7864Z"
                fill="#4215CC"></path>
              <path
                d="M109.399 20.7987V19.8202L110.064 19.7677C110.437 19.7385 110.668 19.6744 110.755 19.5752C110.866 19.4527 110.922 18.9481 110.922 18.0614V10.7551C110.922 9.9559 110.884 9.21381 110.808 9.06214C110.732 8.91048 110.484 8.81714 110.064 8.78214L109.42 8.72V8C110.86 8 111.779 8 112.193 8C112.602 8 113.521 8 114.967 8V8.72L114.308 8.78214C113.929 8.81131 113.695 8.87548 113.608 8.97464C113.503 9.10298 113.45 9.87423 113.45 10.7551V18.0614C113.45 18.8664 113.488 19.3448 113.564 19.4964C113.64 19.6423 113.888 19.7327 114.308 19.7677L114.967 19.8133V20.8C113.654 20.8 112.7 20.7987 112.152 20.7987C111.598 20.7987 110.7 20.7987 109.399 20.7987Z"
                fill="#4215CC"></path>
            </svg>
          </a>

          <!-- Deep Wisdom AI -->
          <a href="https://www.deepwisdom.ai/" target="_blank"
            style="display: flex; align-items: center; text-decoration: none; color: inherit; font-weight: bold; font-size: 55px;">
            <img src="assets/img/mgx.png" alt="Deep Wisdom AI Logo"
              style="max-height: 80px; height: 80px; width: auto; display: inline-block; margin-right: 8px;">
            Deep Wisdom AI
          </a>

        </div>
      </div>


  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><strong>Selecao</strong></strong>.
        All Rights Reserved
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/selecao-bootstrap-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
        and <a href="https://doc2dial.github.io/workshop2022/">DialDoc</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

  <!-- Default Statcounter code for llmagents.github.io
https://llmagents.github.io/ -->
  <script type="text/javascript">
    var sc_project = 12953394;
    var sc_invisible = 1;
    var sc_security = "86f01d45"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12953394/0/86f01d45/1/" alt="Web Analytics"
          referrerPolicy="no-referrer-when-downgrade"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>